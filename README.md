# Customs Channel Prediction Project

This repository contains the complete workflow for building, training, and evaluating machine learning models to predict customs channel (GREEN, YELLOW, RED, GRAY).  
The project follows a structured data science pipeline, from data analysis to model training, evaluation, and prediction generation.

---

## Project Overview

The project develops and evaluates machine learning models (Logistic Regression and Random Forest) to predict customs channel outcomes.  
It includes:
- Exploratory Data Analysis (EDA)
- Feature engineering for categorical and numerical variables  
- Class imbalance handling using hybrid resampling strategies (SMOTE + undersampling)
- Model training with cross-validation and grid search  
- Model evaluation with F1 and balanced accuracy metrics  
- Prediction generation on new unseen data 

---

## Project Structure

| Folder/File | Description |
| :--- | :--- |
| `data/` | This folder contains three datasets: `sample_data.parquet`, which is the raw input data; `data_for_model.csv`, an intermediate preprocessed dataset generated by the notebook `customs_channel_data_analysis.ipynb` for use in `customs_channel_model_training.ipynb` (this file should not be used in the model_training script); and `new_data.parquet`, which is a copy of `sample_data.parquet` created to simulate new data for prediction and used in the model_predict script to demonstrate the complete prediction flow. |
| `visualizations/` | Contains all generated charts and heatmaps from the exploratory data analysis (EDA) done in the `customs_channel_data_analysis.ipynb` notebook. |
| `all_models/` | Stores trained models from the experiments in the training notebook `customs_channel_model_training.ipynb`. |
| `chosen_model/` | Stores the final selected model and necessary preprocessing artifacts (`best_rf_model_resampling.pkl` and `cat_preprocessor.pkl`). |
| `predictions/` | Generated predictions for new data. |
| `logs/` | Contains runtime logs for the command-line scripts (`training.log`, `prediction.log`). |
| `config.json` | Configuration file with model and resampling parameters. |
| `customs_channel_data_analysis.ipynb` | Notebook with exploratory data analysis (EDA) and feature engineering. |
| `customs_channel_model_training.ipynb` | Notebook with model development and evaluation notebook. |
| `model_training.py` | Command-line script to execute the chosen production-ready Random Forest training pipeline. |
| `model_predict.py` | Command-line script for loading the trained model and generating predictions on new data, following the exact same preprocessing steps used during training. |
| `my_functions.py` | Python module containing all reusable functions for data splitting, feature preparation, encoding, scaling, training, and evaluation. |
| `test_my_functions.py` | Unit tests for critical functions. |
| `requirements.txt` | List of all Python dependencies required to run the project. |
| `executive_report.pdf` | Document with the project findings, modelling conclusions, technical documentation of the decisions made, analysis of limitations and next steps. |
| `teste-pleno.pdf` | Project initial instructions. |

---

## Notebooks

| Notebook                               | Description                                                                          |
| -------------------------------------- | ------------------------------------------------------------------------------------ |
| `customs_channel_data_analysis.ipynb`  | Exploratory Data Analysis (EDA) and Feature Engineering notebook. Focuses on data cleaning, handling missing values, temporal/risk feature creation and data analysis. It uses as input the `sample_data.parquet` and produces as output the `data_for_model.csv`, an intermediate preprocessed dataset to be used in the `customs_channel_model_training.ipynb` notebook. |
| `customs_channel_model_training.ipynb` | Model Development and Evaluation notebook. Focuses on training experiments to define the best modelling approach. It uses as input the `data_for_model.csv` dataset. |

---

## How to run the scripts

### 1. Train the model
**Command to run it:**
```bash
python model_training.py
```  
**Inputs and outputs:**  
Loads the raw data `sample_data.parquet`.  
Outputs the best model `best_rf_model_resampling.pkl` along with the categorical preprocessing artifacts `cat_preprocessor.pkl`, which are then saved to the `chosen_model/` directory. 
All logs will be automatically saved in the `logs/` directory.

### 2. Generate predictions on new data

**Command to run it:**
```bash
python model_predict.py
```  
**Inputs and outputs:**  
Loads the new data (`new_data.parquet`) along with the trained model (`best_rf_model_resampling.pkl`) and the `cat_preprocessor.pkl` object from `chosen_model/` directory.  
Outputs the predictions that are saved as CSV file in the `predictions/` directory.  
*Note:* The `new_data.parquet` is not real new data, it is a copy of the raw data `sample_data.parquet` to demonstrate the prediction script.  
All logs will be automatically saved in the `logs/` directory.

### 3. Test critical functions
Unit tests for critical functions.  

**Command to run it:**
```bash
pytest -v test_my_functions.py
```

---

## Prerequisites

1.  Python 3.8+
2.  Install required packages:

```bash
pip install -r requirements.txt
```

---  
  
  

**Author:** Giullia Milano